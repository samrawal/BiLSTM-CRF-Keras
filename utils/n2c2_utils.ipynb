{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, os\n",
    "from word import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class n2c2Parser():    \n",
    "    filepath =  None\n",
    "    tokens = None\n",
    "    tokens_dict = None\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def parse_file(self):\n",
    "        with open(self.filepath, 'r') as f:\n",
    "            data = f.read()\n",
    "        tokens = []\n",
    "        tokens_dict = {}\n",
    "        word_offset = 0\n",
    "        prev_char = ' '\n",
    "        current_token = ''\n",
    "        start_char, end_char = None, None \n",
    "        end_of_word = {' ', '\\n', r','} # tokens signifying end of word\n",
    "        for i, char in enumerate(data):\n",
    "            if prev_char in end_of_word and char not in end_of_word: # new word started\n",
    "                start_char = i\n",
    "                current_token += char\n",
    "            elif prev_char not in end_of_word and char in end_of_word: # word ended\n",
    "                end_char = i\n",
    "                tokens.append(\n",
    "                    {'text': current_token, 'start_char':  start_char, 'end_char':  end_char}\n",
    "                )\n",
    "                tokens_dict[start_char] = {\n",
    "                    'text': current_token,\n",
    "                    'start_char':  start_char,\n",
    "                    'end_char':  end_char\n",
    "                }\n",
    "                current_token = ''\n",
    "            elif prev_char not in end_of_word and char not in end_of_word: # word continues\n",
    "                current_token += char\n",
    "            else: # blank space followed by blank space\n",
    "                pass\n",
    "            prev_char = char\n",
    "        self.tokens = tokens\n",
    "        self.tokens_dict = tokens_dict\n",
    "    \n",
    "    def tag_gold_tokens(self, gold_file):\n",
    "        parsed_gold = {}\n",
    "        with open(gold_file, 'r') as f:\n",
    "            gold_data = f.read().splitlines()\n",
    "        for l in gold_data:\n",
    "            data_in_line = self.parse_gold_line(l)\n",
    "            for data in data_in_line:\n",
    "                tagged_data = False\n",
    "                for token in range(len(self.tokens)):\n",
    "                    if (self.tokens[token]['start_char'] >= data['start_offset'] and\n",
    "                            self.tokens[token]['end_char'] <= data['end_offset']):\n",
    "                        tagged_data = True\n",
    "                        self.tokens[token]['concept'] = data['concept']\n",
    "                        self.tokens[token]['gold_text'] = data['text']\n",
    "                        self.tokens[token]['id'] = data['id']\n",
    "                if not tagged_data:\n",
    "                    print('Data was not tagged for line:\\n{0}'.format(data))\n",
    "\n",
    "                                \n",
    "    def parse_gold_line(self, line):\n",
    "        bundle = []\n",
    "        if line[0] == 'T':\n",
    "            linesplit = line.split()\n",
    "            _id = linesplit[0]\n",
    "            concept = linesplit[1]\n",
    "            if r';' not in linesplit[3]: # continuous\n",
    "                start_offsets = [linesplit[2]]\n",
    "                end_offsets = [linesplit[3]]\n",
    "                text = linesplit[4:]\n",
    "            else: # non-continuous\n",
    "                offsets = ';'.join(linesplit[2:5]).split(';')\n",
    "                start_offsets = [offsets[0], offsets[2]]\n",
    "                end_offsets = [offsets[1], offsets[3]]\n",
    "                text = linesplit[5:]\n",
    "\n",
    "            for start_offset, end_offset in zip(start_offsets, end_offsets):\n",
    "                bundle.append(\n",
    "                    {\n",
    "                        'id': _id,\n",
    "                        'concept': concept,\n",
    "                        'start_offset': int(start_offset),\n",
    "                        'end_offset': int(end_offset),\n",
    "                        'text': ' '.join(text),\n",
    "                    }\n",
    "                )\n",
    "        return bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "input_file = 100039\n",
    "data_path = '/Users/samrawal/Documents/workspace/colab/data/n2c2/track2/training_20180910/'\n",
    "\n",
    "txt_file_path = '{0}/{1}.txt'.format(data_path, input_file)\n",
    "ann_file_path = '{0}/{1}.ann'.format(data_path, input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "parser = n2c2Parser(txt_file_path)\n",
    "parser.parse_file()\n",
    "tokens = parser.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Prochlorperazine', 'start_char': 166, 'end_char': 182}\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    if token['start_char'] == 166:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was not tagged for line:\n",
      "{'id': 'T9', 'concept': 'Drug', 'start_offset': 1383, 'end_offset': 1386, 'text': 'IVF'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T10', 'concept': 'Drug', 'start_offset': 15147, 'end_offset': 15160, 'text': 'anthracycline'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T20', 'concept': 'Drug', 'start_offset': 16574, 'end_offset': 16587, 'text': 'Anthracycline'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T22', 'concept': 'ADE', 'start_offset': 16602, 'end_offset': 16616, 'text': 'cardiomyopathy'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T25', 'concept': 'Drug', 'start_offset': 17039, 'end_offset': 17048, 'text': 'milrinone'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T26', 'concept': 'Drug', 'start_offset': 17049, 'end_offset': 17054, 'text': 'lasix'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T30', 'concept': 'Route', 'start_offset': 17240, 'end_offset': 17243, 'text': 'gtt'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T36', 'concept': 'Reason', 'start_offset': 17473, 'end_offset': 17477, 'text': 'GVHD'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T47', 'concept': 'Reason', 'start_offset': 18154, 'end_offset': 18158, 'text': 'GVHD'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T54', 'concept': 'Drug', 'start_offset': 1902, 'end_offset': 1913, 'text': 'hydroxyurea'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T55', 'concept': 'Drug', 'start_offset': 19058, 'end_offset': 19067, 'text': 'milrinone'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T56', 'concept': 'Drug', 'start_offset': 19068, 'end_offset': 19073, 'text': 'lasix'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T63', 'concept': 'Drug', 'start_offset': 19326, 'end_offset': 19335, 'text': 'torsemide'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T64', 'concept': 'Reason', 'start_offset': 1943, 'end_offset': 1946, 'text': 'ALL'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T91', 'concept': 'Drug', 'start_offset': 2041, 'end_offset': 2044, 'text': 'MTX'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T132', 'concept': 'Reason', 'start_offset': 20889, 'end_offset': 20895, 'text': 'nausea'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T140', 'concept': 'Reason', 'start_offset': 21057, 'end_offset': 21063, 'text': 'wheeze'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T149', 'concept': 'Reason', 'start_offset': 21146, 'end_offset': 21152, 'text': 'nausea'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T173', 'concept': 'Frequency', 'start_offset': 21611, 'end_offset': 21614, 'text': 'four times a day'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T210', 'concept': 'Reason', 'start_offset': 22197, 'end_offset': 22200, 'text': 'gas'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T232', 'concept': 'Reason', 'start_offset': 22566, 'end_offset': 22578, 'text': 'constipation'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T240', 'concept': 'Reason', 'start_offset': 22696, 'end_offset': 22700, 'text': 'pain'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T248', 'concept': 'Drug', 'start_offset': 23301, 'end_offset': 23307, 'text': 'Bentyl'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T249', 'concept': 'Drug', 'start_offset': 23365, 'end_offset': 23373, 'text': 'morphine'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T255', 'concept': 'Reason', 'start_offset': 24234, 'end_offset': 24242, 'text': 'leukemia'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T256', 'concept': 'Drug', 'start_offset': 2429, 'end_offset': 2442, 'text': 'anthracycline'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T261', 'concept': 'Drug', 'start_offset': 24534, 'end_offset': 24544, 'text': 'Prednisone'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T264', 'concept': 'Drug', 'start_offset': 24569, 'end_offset': 24577, 'text': 'Coumadin'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T267', 'concept': 'Drug', 'start_offset': 24606, 'end_offset': 24615, 'text': 'Torsemide'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T275', 'concept': 'Drug', 'start_offset': 24775, 'end_offset': 24781, 'text': 'Bentyl'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T276', 'concept': 'Drug', 'start_offset': 24783, 'end_offset': 24794, 'text': 'dicyclomine'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T278', 'concept': 'Drug', 'start_offset': 24833, 'end_offset': 24844, 'text': 'Simethicone'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T280', 'concept': 'Reason', 'start_offset': 24882, 'end_offset': 24885, 'text': 'gas'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T285', 'concept': 'Drug', 'start_offset': 24956, 'end_offset': 24963, 'text': 'Bactrim'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T291', 'concept': 'Drug', 'start_offset': 25036, 'end_offset': 25045, 'text': 'Acyclovir'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T295', 'concept': 'Drug', 'start_offset': 25103, 'end_offset': 25114, 'text': 'Allopurinol'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T299', 'concept': 'Drug', 'start_offset': 25450, 'end_offset': 25458, 'text': 'coumadin'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T300', 'concept': 'Drug', 'start_offset': 506, 'end_offset': 519, 'text': 'anthracycline'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T304', 'concept': 'Drug', 'start_offset': 8177, 'end_offset': 8185, 'text': 'contrast'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T15', 'concept': 'Drug', 'start_offset': 6398, 'end_offset': 6406, 'text': 'contrast'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T309', 'concept': 'Form', 'start_offset': 9533, 'end_offset': 9539, 'text': 'tablet'}\n",
      "Data was not tagged for line:\n",
      "{'id': 'T312', 'concept': 'Reason', 'start_offset': 17147, 'end_offset': 17154, 'text': 'ascites'}\n"
     ]
    }
   ],
   "source": [
    "line = 'T80\tReason 17807 17824;17825 17837\tparoxysmal atrial fibrillation'\n",
    "line2 = 'T75\tDrug 17749 17758\tquinidine '\n",
    "parser = n2c2Parser(txt_file_path)\n",
    "parser.parse_file()\n",
    "parser.tag_gold_tokens(ann_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'IVF.', 'start_char': 1383, 'end_char': 1387}\n"
     ]
    }
   ],
   "source": [
    "for t in parser.tokens:\n",
    "    if t['start_char'] == 1383:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file_path, 'r') as f: data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "n2c2_utils.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
