{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, os\n",
    "from word import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class n2c2Parser():    \n",
    "    filepath =  None\n",
    "    tokens = None\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def parse_file(self):\n",
    "        with open(self.filepath, 'r') as f:\n",
    "            data = f.read()\n",
    "        tokens = []\n",
    "        word_offset = 0\n",
    "        prev_char = ' '\n",
    "        current_token = ''\n",
    "        start_char, end_char = None, None \n",
    "        end_of_word = {' ', '\\n'} # tokens signifying end of word\n",
    "        for i, char in enumerate(data):\n",
    "            if prev_char in end_of_word and char not in end_of_word: # new word started\n",
    "                start_char = i\n",
    "                current_token += char\n",
    "            elif prev_char not in end_of_word and char in end_of_word: # word ended\n",
    "                end_char = i\n",
    "                tokens.append(\n",
    "                    {'text': current_token, 'start_char':  start_char, 'end_char':  end_char}\n",
    "                )\n",
    "                current_token = ''\n",
    "            elif prev_char not in end_of_word and char not in end_of_word: # word continues\n",
    "                current_token += char\n",
    "            else: # blank space followed by blank space\n",
    "                pass\n",
    "            prev_char = char\n",
    "        self.tokens = tokens\n",
    "    \n",
    "    def tag_gold_tokens(self, gold_file):\n",
    "        parsed_gold = {}\n",
    "        with open(gold_file, 'r') as f:\n",
    "            gold_data = f.read().splitlines()\n",
    "        for l in gold_data:\n",
    "            data_in_line = self.parse_gold_line(l)\n",
    "            for data in data_in_line:\n",
    "                if data['start_line'] != None:\n",
    "                    for line in range(data['start_line'], data['end_line']+1):\n",
    "                        if data['start_line'] == data['end_line']:\n",
    "                            for token in range(data['start_token'], data['end_token'] + 1):\n",
    "                                self.tokens[line][token]['gold_tag'] = data['concept']\n",
    "                                self.tokens[line][token]['gold_text'] = data['text']\n",
    "                        else: # if multi-line annotation\n",
    "                            if line == data['start_line']:\n",
    "                                for token in self.tokens[line]:\n",
    "                                    if token >= data['start_token']: \n",
    "                                        self.tokens[line][token]['gold_tag'] = data['concept']\n",
    "                                        self.tokens[line][token]['gold_text'] = data['text']\n",
    "                            elif line == data['end_line']:\n",
    "                                for token in self.tokens[line]:\n",
    "                                    if token <= data['end_token']: \n",
    "                                        self.tokens[line][token]['gold_tag'] = data['concept']\n",
    "                                        self.tokens[line][token]['gold_text'] = data['text']\n",
    "                            else:\n",
    "                                for token in self.tokens[line]:\n",
    "                                    self.tokens[line][token]['gold_tag'] = data['concept']\n",
    "                                    self.tokens[line][token]['gold_text'] = data['text']\n",
    "\n",
    "                                \n",
    "    def parse_gold_line(self, line):\n",
    "        bundle = []\n",
    "        chunks = line.split('||')\n",
    "        for chunk in chunks:\n",
    "            split_a = chunk.split('\\\" ')\n",
    "            concept = split_a[0].split('=')[0]\n",
    "            text = split_a[0].split('=')[1][1:]\n",
    "            \n",
    "            split_b = split_a[1].split() if len(split_a) > 1 else []\n",
    "            if len(split_b) == 0: # no start/end values here\n",
    "                start_line, start_token = None, None\n",
    "                end_line, end_token = None, None\n",
    "                bundle.append({\n",
    "                    'concept': concept,\n",
    "                    'text': text,\n",
    "                    'start_line': start_line,\n",
    "                    'start_token': start_token,\n",
    "                    'end_line': end_line,\n",
    "                    'end_token': end_token,\n",
    "                })\n",
    "\n",
    "            elif r',' not in split_b[1]:\n",
    "                [start_line, start_token] = split_b[0].split(':')\n",
    "                [end_line, end_token] = split_b[1].split(':')\n",
    "                bundle.append({\n",
    "                    'concept': concept,\n",
    "                    'text': text,\n",
    "                    'start_line': int(start_line),\n",
    "                    'start_token': int(start_token),\n",
    "                    'end_line': int(end_line),\n",
    "                    'end_token': int(end_token),\n",
    "                })\n",
    "            else:\n",
    "                groups = split_a[1].split(r',')\n",
    "                for group in groups:\n",
    "                    [start, end] = group.split()\n",
    "                    [start_line, start_token] = start.split(':')\n",
    "                    [end_line, end_token] = end.split(':')\n",
    "                    bundle.append({\n",
    "                        'concept': concept,\n",
    "                        'text': text,\n",
    "                        'start_line': int(start_line),\n",
    "                        'start_token': int(start_token),\n",
    "                        'end_line': int(end_line),\n",
    "                        'end_token': int(end_token),\n",
    "                    })\n",
    "        return bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "input_file = 100039\n",
    "data_path = '/Users/samrawal/Documents/workspace/colab/data/n2c2/track2/training_20180910/'\n",
    "\n",
    "txt_file_path = '{0}/{1}.txt'.format(data_path, input_file)\n",
    "ann_file_path = '{0}/{1}.ann'.format(data_path, input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "parser = n2c2Parser(txt_file_path)\n",
    "parser.parse_file()\n",
    "tokens = parser.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Prochlorperazine', 'start_char': 166, 'end_char': 182}\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    if token['start_char'] == 166:\n",
    "        print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "n2c2_utils.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
